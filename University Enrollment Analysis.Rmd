---
title: "Project 2 University Enrollment"
author  : Xuhui Ying
date    : 10/05/2022 
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: paper
    highlight: tango
    df_print: paged
---

# Logistic Models, the Tidymodel way. 

## Load Libraries 

```{r, eval=TRUE, warning=FALSE, message=FALSE}
options(warn = -1)
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(kableExtra)
library(GGally)
library(kableExtra) # -- make nice looking resutls when we knitt 
library(vip)        # --  tidymodels variable importance
library(fastshap)   # -- shapley values for variable importance 
library(MASS)
```

## Stage w. Readr

Import your data with read_csv()

```{r, eval=TRUE, warning=FALSE, message=FALSE}
inq05_samp <- read_csv("inq05_samp.csv") %>%
  clean_names()

df <- inq05_samp %>% dplyr::select(-academic_interest_1, -academic_interest_2, -contact_code1, -contact_date1, -ethn_code, -irschool, -level_year, -satscore, -sex, -telecq, -referral_cntcts, -recr_code)

nrow(df)

head(df) 
```

## Profile w. Skimr 

skimr has lots of options and supports groupby - check it out. 

```{r, eval=TRUE, warning=FALSE, message=FALSE}

df %>%
  skim()

# address missing values (drop columns with more than 20% missing values)

data <- df %>% dplyr::select(-avg_income, -distance)

#data %>% write_csv("data.csv")
```

## Explore target

what's the frequency of responders? remember the count function of summarize is n() with no parameters. 

```{r, eval=TRUE, warning=FALSE, message=FALSE}
data$enroll <- as.factor(data$enroll)

data %>%
  ggplot(aes(x=enroll, fill=enroll)) +
  geom_histogram(stat="count") +
  labs(title = "enroll vs not enroll")

data %>%
  group_by(enroll) %>%
  summarize(n=n()) %>%
  ungroup() %>%
  mutate(pct = n/sum(n))
```

## Explore numerics 

Compare numeric variables by comparing histograms of respond vs non-respond of course you can follow up with descriptive statistics too...

numeric variables: total_contacts, self_init_cntcts, travel_init_cntcts, solicited_cntcts, referral_cntcts, mailq, interest, init_span, int1rat, int2rat, hscrat

```{r, eval=TRUE, warning=FALSE, message=FALSE}

# -- comparative boxplots

boxplot <- function(m){
    ggplot(data, aes(x=!!as.name(m), y=enroll, fill=enroll)) + 
    geom_boxplot() +
    labs(title = as.character(m))
}

numerics <- c("total_contacts", "self_init_cntcts", "travel_init_cntcts", "solicited_cntcts", "mailq", "interest", "init_span", "int1rat", "int2rat", "hscrat")

for (c in numerics){
    print(boxplot(c))
}

```

## Explore character variables  

cycle through each character column and look for separation for respond vs non-respond

categorical variables: recr_code, campus_visit, premiere, stuemail, instate

```{r, eval=TRUE, warning=FALSE, message=FALSE}

char_explore <- function(col){
    data %>%
    ggplot(aes(!!as.name(col))) + 
    geom_bar(aes(fill = enroll), position = "fill") 
}

data$campus_visit <- as.character(data$campus_visit)
data$premiere <- as.character(data$premiere)
data$stuemail <- as.character(data$stuemail)

# -- for each character column, create a chart
for (column in names(data %>% select_if (is_character))){
    chrt <- char_explore(column)
    print(chrt)
}

```

## 0. Make Factors! 

The next step for us is to create a dataset for modeling. Letâ€™s include a set of all of the columns we are interested in, and convert all the **character columns** to **factors** as well as any "nominal" or  low frequency numeric columns likely to be a factor. This is done for the modeling functions coming later.  

```{r, eval=TRUE, warning=FALSE, message=FALSE}
data %>%
    mutate_if(is.character, factor)  -> data_prep

head(data_prep)

```

## 1. Partition my data 70/30 (train / test split) 

```{r, eval=TRUE, warning=FALSE, message=FALSE}
# -- set a random seed for repeatablity 
set.seed(1234)

# -- performs our train / test split 
data_split <- initial_split(data_prep, prop = 0.7)

# -- extract the training data 
data_train <- training(data_split)
# -- extract the test data 
data_test <- testing(data_split)

sprintf("Train PCT : %1.2f%%", nrow(data_train)/ nrow(data) * 100)
sprintf("Test  PCT : %1.2f%%", nrow(data_test)/ nrow(data) * 100)

```

## 2. Recipe


```{r, eval=TRUE, warning=FALSE, message=FALSE}
# -- create our recipe -- 
data_recipe <- recipe(enroll ~ ., data = data_train) %>%
#  step_rm(duration) %>%
  step_impute_mode(all_nominal(), -all_outcomes()) %>%
  step_impute_median(all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  prep()

data_recipe
```

## 3. Bake 


```{r, eval=TRUE, warning=FALSE, message=FALSE}
# -- apply the recipe 
bake_train <- bake(data_recipe, new_data = data_train)
bake_test  <- bake(data_recipe, new_data = data_test)
```

## 4. Fit 

```{r, eval=TRUE, warning=FALSE, message=FALSE}
## logistic code is here for reference and comparison

logistic_glm <-logistic_reg(mode = "classification") %>%
                  set_engine("glm") %>%
                  fit(enroll ~ ., data = bake_train)

## -- check out your parameter estimates ... 
tidy(logistic_glm) %>%
  mutate_at(c("estimate", "std.error", "statistic", "p.value"),round, 4)

```

## 5. Prep for Evaluation 

We want to attach both the Predicted Probabilities (.pred_No, .pred_Yes) and the Predicted Class (.pred_class) to the dataset so we can deep dive into where out model is performing well and where it's not. We do this to both the Training and the Test set. 

```{r, eval=TRUE, warning=FALSE, message=FALSE}

# -- training 
predict(logistic_glm, bake_train, type = "prob") %>%
  bind_cols(.,predict(logistic_glm, bake_train)) %>%
  bind_cols(.,bake_train) -> scored_train_glm

head(scored_train_glm)

# -- testing 
predict(logistic_glm, bake_test, type = "prob") %>%
  bind_cols(.,predict(logistic_glm, bake_test)) %>%
  bind_cols(.,bake_test) -> scored_test_glm

head(scored_test_glm)
```

## 6. Evaluate

We want to check our model's performance and take a look at which features were most important. 

```{r, eval=TRUE, warning=FALSE, message=FALSE}

options(yardstick.event_first = FALSE)

# -- AUC: Train and Test 
scored_train_glm %>% 
  metrics(enroll, .pred_1, estimate = .pred_class) %>%
  mutate(part="training") %>%
  bind_rows( scored_test_glm %>% 
               metrics(enroll, .pred_1, estimate = .pred_class) %>%
               mutate(part="testing") 
  ) %>%
  filter(.metric %in% c("accuracy", "roc_auc"))

# -- Variable Importance top 10 features  
logistic_glm %>%
  vip(num_features = 10)

# -- ROC Charts 
scored_train_glm %>%
  mutate(model = "train") %>%
  bind_rows(scored_test_glm %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(enroll, .pred_1) %>%
  autoplot()


# -- Confustion Matricies  
scored_train_glm %>%
  conf_mat(enroll, .pred_class) %>%
  autoplot( type = "heatmap") +
  labs(title="Train Confusion Matrix")

scored_test_glm %>%
  conf_mat(enroll, .pred_class) %>%
  autoplot( type = "heatmap") +
  labs(title="Test Confusion Matrix")

```


```{r, eval=TRUE, warning=FALSE, message=FALSE}

## -- Use stepwise selection to reduce the model

steplog <- glm(enroll ~ ., data = bake_train, family=binomial(link="logit"))
step <- stepAIC(steplog, direction="both")
summary(step)

```

```{r, eval=TRUE, warning=FALSE, message=FALSE}
## -- remove insignificant variable (stuemail)

model_1 <- glm(enroll ~ total_contacts + self_init_cntcts + solicited_cntcts + mailq + interest + init_span + int1rat + int2rat + hscrat + campus_visit + premiere + instate, data = data_train, family=binomial(link="logit"))

model_1
summary(model_1)

```

```{r, eval=TRUE, warning=FALSE, message=FALSE}

## -- Use tidymodel framework to fit and evaulate reduced model

uni_steprecipe <- recipe(enroll ~ total_contacts + self_init_cntcts + solicited_cntcts + mailq + interest + init_span + int1rat + int2rat + hscrat + campus_visit + premiere + instate, data = data_train) %>%
    step_impute_median(all_numeric()) %>%
    prep()

uni_steprecipe

# -- apply new recipe 
bake_steptrain <- bake(uni_steprecipe, new_data = data_train)
bake_steptest  <- bake(uni_steprecipe, new_data = data_test)

logistic_step1 <- logistic_reg(mode = "classification") %>%
                  set_engine("glm") %>%
                  fit(enroll ~ ., data = bake_steptrain)


## -- check out your parameter estimates ...
tidy(logistic_step1) %>%
  mutate_at(c("estimate", "std.error", "statistic", "p.value"),round, 4)

```

```{r, eval=TRUE, warning=FALSE, message=FALSE}
# -- training predictions from stepwise model
predict(logistic_step1, bake_steptrain, type = "prob") %>%
  bind_cols(.,predict(logistic_step1, bake_steptrain)) %>%
  bind_cols(.,bake_steptrain) -> scored_train_step1

head(scored_train_step1)

# -- testing predictions from stepwise model
predict(logistic_step1, bake_steptest, type = "prob") %>%
  bind_cols(.,predict(logistic_step1, bake_steptest)) %>%
  bind_cols(.,bake_steptest) -> scored_test_step1

head(scored_test_step1)
```

```{r, eval=TRUE, warning=FALSE, message=FALSE}

# -- Evaluate Stepwise Model
# -- AUC: Train and Test 
options(yardstick.event_first = FALSE)

scored_train_step1 %>% 
  metrics(enroll, .pred_1, estimate = .pred_class) %>%
  mutate(part="training") %>%
  bind_rows( scored_test_step1 %>% 
               metrics(enroll, .pred_1, estimate = .pred_class) %>%
               mutate(part="testing") 
  ) %>%
  filter(.metric %in% c("accuracy", "roc_auc"))


# -- Variable Importance top 10 features  
model_1 %>%
  vip(num_features = 10)


# -- ROC Charts 
scored_train_step1 %>%
  mutate(model = "train") %>%
  bind_rows(scored_test_step1 %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(enroll, .pred_1) %>%
  autoplot()


# -- Confustion Matricies  
scored_train_step1 %>%
  conf_mat(enroll, .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title="Train Confusion Matrix")

scored_test_step1 %>%
  conf_mat(enroll, .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title="Test Confusion Matrix")

```

